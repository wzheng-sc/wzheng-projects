{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daily Viral Places Classification (sequential by day)\n",
        "# This notebook processes one day at a time from START_DATE to END_DATE.\n",
        "\n",
        "import os\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "from tqdm.contrib.concurrent import thread_map\n",
        "from google.cloud import storage\n",
        "\n",
        "from banjo import utils\n",
        "from utils.query import get_viral_places_query\n",
        "from utils.helper import (\n",
        "    upload_url_to_gcs,\n",
        "    process_urls_threaded,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "PROJECT_ID   = \"myaigcp\"\n",
        "BUCKET_NAME  = \"shiba-inu-temp\"\n",
        "BUCKET_ROOT  = \"maps_stories_daily\"\n",
        "\n",
        "# Date range (inclusive)\n",
        "START_DATE = \"2025-07-01\"  # YYYY-MM-DD\n",
        "END_DATE   = \"2025-07-03\"  # YYYY-MM-DD\n",
        "\n",
        "# Scoring weights (used by get_viral_places_query)\n",
        "VIEW_WEIGHT      = 0.6\n",
        "FRESHNESS_WEIGHT = 0.4\n",
        "\n",
        "# Make GCP project discoverable, construct Storage client\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers for date iteration\n",
        "\n",
        "def iter_dates(start_date: str, end_date: str):\n",
        "    start = dt.datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "    end   = dt.datetime.strptime(end_date,   \"%Y-%m-%d\").date()\n",
        "    cur = start\n",
        "    while cur <= end:\n",
        "        yield cur\n",
        "        cur = cur + dt.timedelta(days=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification setup (configure as needed)\n",
        "from banjo.utils.shibainu import Classification, configure_logger\n",
        "from utils.prompt import PROMPT\n",
        "\n",
        "configure_logger()\n",
        "\n",
        "# Minimal example config; adjust provider/model as needed\n",
        "classifier = Classification(\n",
        "    provider_name=\"openai\",\n",
        "    model_name='gpt-4o-mini',\n",
        "    input_type=\"video\",\n",
        "    provider_config={},\n",
        "    processor_config={\n",
        "        \"processing_mode\": \"image_url\",\n",
        "        \"download\": True,\n",
        "        \"sampling_mode\": \"fps\",\n",
        "        \"sampling_value\": 1.0,\n",
        "        \"max_frames\": 20,\n",
        "        \"image_quality\": 75,\n",
        "        \"image_format\": \"JPEG\",\n",
        "    },\n",
        "    model_parameters={\n",
        "        \"temperature\": 0,\n",
        "        \"max_token\": 128,\n",
        "    },\n",
        "    prompt=PROMPT,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main per-day loop\n",
        "\n",
        "from utils.helper import parse_incident_json_broken\n",
        "\n",
        "all_days = []  # collect daily outputs if desired\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "for day in iter_dates(START_DATE, END_DATE):\n",
        "    ymd = day.strftime('%Y%m%d')\n",
        "    print(f\"\\n=== Processing day {ymd} ===\")\n",
        "\n",
        "    # Per-day bucket subfolder\n",
        "    bucket_folder = f\"{BUCKET_ROOT}/{ymd}\"\n",
        "\n",
        "    # 1) Query data for this day\n",
        "    query = get_viral_places_query(\n",
        "        start_date=ymd,\n",
        "        end_date=ymd,\n",
        "        view_weight=VIEW_WEIGHT,\n",
        "        freshness_weight=FRESHNESS_WEIGHT,\n",
        "    )\n",
        "    df = utils.gbq.read_gbq(\n",
        "        query,\n",
        "        project_id=PROJECT_ID,\n",
        "        dialect=\"standard\",\n",
        "        priority=\"interactive\",\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"No rows for {ymd} — skipping\")\n",
        "        continue\n",
        "\n",
        "    # 2) Filter non-null URL rows\n",
        "    df = df[df['media_url'].notna()].copy()\n",
        "    if df.empty:\n",
        "        print(f\"No media_url for {ymd} — skipping\")\n",
        "        continue\n",
        "\n",
        "    # 3) Upload original media URLs to GCS (keep original media_url for classification)\n",
        "    urls = df['media_url'].tolist()\n",
        "    ids  = df['story_snap_id'].tolist()\n",
        "    df['gcs_url'] = process_urls_threaded(\n",
        "        urls, ids,\n",
        "        bucket_name=BUCKET_NAME,\n",
        "        bucket_folder=bucket_folder,\n",
        "        storage_client=storage_client,\n",
        "        max_workers=10,\n",
        "    )\n",
        "\n",
        "    # 4) Classify using original media_url to avoid signing gs://\n",
        "    try:\n",
        "        cls_results = thread_map(classifier.classify, df['media_url'].tolist(), max_workers=5)\n",
        "        df['labels'] = [classifier.get_result(r) for r in cls_results]\n",
        "        df['prompt_tokens'] = [classifier.get_token_usage(r).get('prompt_tokens', 0) for r in cls_results]\n",
        "        df['completion_tokens'] = [classifier.get_token_usage(r).get('completion_tokens', 0) for r in cls_results]\n",
        "\n",
        "        # Parse labels into structured columns\n",
        "        parsed = df['labels'].apply(parse_incident_json_broken)\n",
        "        parsed_df = pd.DataFrame(list(parsed)) if len(parsed) else pd.DataFrame()\n",
        "        if not parsed_df.empty:\n",
        "            for col in parsed_df.columns:\n",
        "                df[col] = parsed_df[col]\n",
        "    except Exception as e:\n",
        "        print(f\"Classification failed for {ymd}: {e}\")\n",
        "\n",
        "    # 5) Save per-day CSV to GCS\n",
        "    try:\n",
        "        tmp_path = f\"/tmp/{ymd}_classified.csv\"\n",
        "        df.to_csv(tmp_path, index=False)\n",
        "        blob = bucket.blob(f\"{bucket_folder}/classified.csv\")\n",
        "        blob.upload_from_filename(tmp_path, content_type='text/csv')\n",
        "        os.unlink(tmp_path)\n",
        "        print(f\"Saved gs://{BUCKET_NAME}/{bucket_folder}/classified.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"Save failed for {ymd}: {e}\")\n",
        "\n",
        "    # 6) Save or collect per-day results\n",
        "    all_days.append(df)\n",
        "\n",
        "# Optional: stitch all days\n",
        "df_all_days = pd.concat(all_days, ignore_index=True) if all_days else pd.DataFrame()\n",
        "print(f\"\\nCompleted. Total rows across days: {len(df_all_days)}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
